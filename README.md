# gpt-from-scratch(somewhat)

This was more of a learning project than anything I wanted to showcase, to learn how LLMs worked under the hood building all the components from scratch. The outputs given are extremely inaccurate and this can't really be used for anything because it's untrained and not fine-tuned to be able to use for general cases. I'm not gonna be working on that part of it, and this was a simple way for me to explore the transformer architecture to understand how GPT works.

The `config.py` file contains actual parameters of GPT-2 if you're interested but yeah, just leaving this here incase anyone's interested in how it works.

The tokenizer, attention mechanisms and embeddings are there within the `concepts` folder, while the actual transformer model and GPT implementation are in the root.
